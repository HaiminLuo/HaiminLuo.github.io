<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>1 | Haimin Luo</title>
    <link>https://HaiminLuo.github.io/publication-type/1/</link>
      <atom:link href="https://HaiminLuo.github.io/publication-type/1/index.xml" rel="self" type="application/rss+xml" />
    <description>1</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 28 Sep 2021 16:40:59 +0800</lastBuildDate>
    <image>
      <url>https://HaiminLuo.github.io/media/icon_hufc8fad64bc5d645be9bad719311346b8_550_512x512_fill_lanczos_center_3.png</url>
      <title>1</title>
      <link>https://HaiminLuo.github.io/publication-type/1/</link>
    </image>
    
    <item>
      <title>Few-shot Neural Human Performance Rendering from Sparse RGBD Videos</title>
      <link>https://HaiminLuo.github.io/publication/fnhr/</link>
      <pubDate>Tue, 28 Sep 2021 16:40:59 +0800</pubDate>
      <guid>https://HaiminLuo.github.io/publication/fnhr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Convolutional Neural Opacity Radiance Fields</title>
      <link>https://HaiminLuo.github.io/publication/convnerf/</link>
      <pubDate>Sat, 04 Sep 2021 00:28:42 +0800</pubDate>
      <guid>https://HaiminLuo.github.io/publication/convnerf/</guid>
      <description>













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/convnerf/featured_hu72c45eabdeeb158d6e8053aea787965a_11757868_d877f64ad7cdb4bfd258edcfebdb2c25.webp 400w,
               /publication/convnerf/featured_hu72c45eabdeeb158d6e8053aea787965a_11757868_7c834461984cff73bf55e80fd3b02c27.webp 760w,
               /publication/convnerf/featured_hu72c45eabdeeb158d6e8053aea787965a_11757868_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://HaiminLuo.github.io/publication/convnerf/featured_hu72c45eabdeeb158d6e8053aea787965a_11757868_d877f64ad7cdb4bfd258edcfebdb2c25.webp&#34;
               width=&#34;1400&#34;
               height=&#34;494&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/JmMwExABY_0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;p&gt;Given multi-view RGBA images, we use an SFS to infer proxy geometric for Efficient Ray Sampling. For each sample point in the volume space, the position and direction are feeding to an MLP based feature prediction network to represent the object at a global level. We next concatenate nearby rays into local feature patches and decoded them into RGB and matte with the convolutional volume renderer. An adversarial training strategy is used on the final output to encourage fine surface details. In the reference period, we render the entire image at once rather than per patch rendering.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/convnerf/pipeline_hu12c7ef6c0cc8d08414f4b3a0f0b29ad3_260898_a4d40d63cff600001ad9584ef6d39797.webp 400w,
               /publication/convnerf/pipeline_hu12c7ef6c0cc8d08414f4b3a0f0b29ad3_260898_d48abd1aa8e423fcbba19ba3fe580984.webp 760w,
               /publication/convnerf/pipeline_hu12c7ef6c0cc8d08414f4b3a0f0b29ad3_260898_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://HaiminLuo.github.io/publication/convnerf/pipeline_hu12c7ef6c0cc8d08414f4b3a0f0b29ad3_260898_a4d40d63cff600001ad9584ef6d39797.webp&#34;
               width=&#34;1400&#34;
               height=&#34;225&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h1 id=&#34;our-results&#34;&gt;Our Results&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/YrdsulYTXzA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Gnerf: Gan-based neural radiance field without posed camera</title>
      <link>https://HaiminLuo.github.io/publication/gnerf/</link>
      <pubDate>Mon, 16 Aug 2021 16:41:06 +0800</pubDate>
      <guid>https://HaiminLuo.github.io/publication/gnerf/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
