
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"PhD student in Visual and Data Intelligence Center(VDI), School of Information Science and Technology(SIST), ShanghaiTech University.\nI am a Second-year PhD student in Computer Science at ShanghaiTech University, working with Prof. Jingyi Yu. I also work closely with Prof. Lan Xu. I received my bachelor’s degree in 2019 from Huazhong University of Science and Technology.\nMy research interest lies in Neural Modeling and Rendering, 3D Reconstruction, Motion Capture and Animation, Digital Human and Animals. Download my resumé .\n","date":1708038864,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1708038864,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"PhD student in Visual and Data Intelligence Center(VDI), School of Information Science and Technology(SIST), ShanghaiTech University.\nI am a Second-year PhD student in Computer Science at ShanghaiTech University, working with Prof.","tags":null,"title":"Haimin Luo","type":"authors"},{"authors":["Haimin Luo","Min Ouyang","Zijun Zhao","Suyi Jiang","Longwen Zhang","Qixuan Zhang","Wei Yang","Lan Xu","Jingyi Yu"],"categories":[],"content":"","date":1708038864,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708038864,"objectID":"9a0aa04ea2b0c61d99da6a5d066426e9","permalink":"http://localhost:1313/publication/gaussianhair/","publishdate":"2024-02-16T07:14:25+08:00","relpermalink":"/publication/gaussianhair/","section":"publication","summary":"This paper presents GaussianHair, a novel explicit hair representation. It enables comprehensive modeling of hair geometry and appearance from images, fostering innovative illumination effects and dynamic animation capabilities.","tags":[],"title":"GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians","type":"publication"},{"authors":["Suyi Jiang","Haimin Luo","Haoran Jiang","Ziyu Wang","Jingyi Yu","Lan Xu"],"categories":[],"content":" ","date":1702612586,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702612586,"objectID":"dd124985a5caa66e981ee31e2ca89865","permalink":"http://localhost:1313/publication/mvhuman/","publishdate":"2023-12-15T11:56:27+08:00","relpermalink":"/publication/mvhuman/","section":"publication","summary":"We present an alternative scheme named MVHuman to generate human radiance fields from text guidance, with consistent multi-view images directly sampled from pre-trained Stable Diffsuions without any fine-tuning or distilling.","tags":[],"title":"MVHuman: Tailoring 2D Diffusion with Multi-view Sampling For Realistic 3D Human Generation","type":"publication"},{"authors":["Suyi Jiang","Haoran Jiang","Ziyu Wang","Haimin Luo","Wenzheng Chen","Lan Xu"],"categories":[],"content":" ","date":1686153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686153600,"objectID":"f3b3ef954a192d816675b82f5144cce0","permalink":"http://localhost:1313/publication/humangen/","publishdate":"2023-06-08T00:00:01+08:00","relpermalink":"/publication/humangen/","section":"publication","summary":"We introduce a hybrid feature representation using the anchor image to bridge the latent space of HumanGen with the existing 2D generator. We then adopt a pronged design to disentangle the generation of geometry and appearance.","tags":[],"title":"HumanGen: Generating Human Radiance Fields with Explicit Priors","type":"publication"},{"authors":["Yuheng Jiang","Kaixin Yao","Zhuo Su","Zehao Shen","Haimin Luo","Lan Xu"],"categories":[],"content":" ","date":1686153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686153600,"objectID":"67fd580960895059c97e303cd451abf6","permalink":"http://localhost:1313/publication/instantnvr/","publishdate":"2023-06-08T00:00:01+08:00","relpermalink":"/publication/instantnvr/","section":"publication","summary":"In this paper, we propose Instant-NVR, a neural approach for instant volumetric human-object tracking and rendering using a single RGBD camera.","tags":[],"title":"Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream","type":"publication"},{"authors":["Juze Zhang","Haimin Luo","Hongdi Yang","Xinru Xu","Qianyang Wu","Ye Shi","Jingyi Yu","Lan Xu","Jingya Wang"],"categories":[],"content":" ","date":1686153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686153600,"objectID":"9500bd0a19eeb9854d56650d3c901aef","permalink":"http://localhost:1313/publication/neuraldome/","publishdate":"2023-06-08T00:00:01+08:00","relpermalink":"/publication/neuraldome/","section":"publication","summary":"We construct a dense multi-view dome to acquire a complex human object interaction dataset, named HODome, that consists of ∼71M frames on 10 subjects interacting with 23 objects. To process the HODome dataset, we develop NeuralDome, a layer-wise neural processing pipeline tailored for multi-view video inputs to conduct accurate tracking, geometry reconstruction and free-view rendering, for both human subjects and objects.","tags":[],"title":"NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions","type":"publication"},{"authors":["Haimin Luo","Siyuan Zhang","Fuqiang Zhao","Haotian Jing","Penghao Wang","Zhenxiao Yu","Dongxue Yan","Junran Ding","Boyuan Zhang","Qiang Hu","Shu Yin","Lan Xu","Jingyi Yu"],"categories":[],"content":"","date":1678182453,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678182453,"objectID":"d8b7163e766e572ebe3b113e2a32d942","permalink":"http://localhost:1313/publication/nephele/","publishdate":"2023-03-07T17:47:33+08:00","relpermalink":"/publication/nephele/","section":"publication","summary":"In this paper, we resort to cloud rendering and present NEPHELE, a neural platform for highly realistic cloud radiance rendering. In stark contrast with existing NR approaches, our NEPHELE allows for more powerful rendering capabilities by combining multiple remote GPUs, and facilitates collaboration by allowing multiple people to view the same NeRF scene simultaneously.","tags":[],"title":"NEPHELE: A Neural Platform for Highly Realistic Cloud Radiance Rendering","type":"publication"},{"authors":["Haimin Luo","Teng Xu","Yuheng Jiang","Chenglin Zhou","Qiwei Qiu","Yingliang Zhang","Wei Yang","Lan Xu","Jingyi Yu"],"categories":null,"content":" Abstract We human are entering into a virtual era, and surely want to bring animals to virtual world as well for companion. Human are entering into a virtual era, while our companions, i.e., animals, are left bind. Yet, computer-generated (CGI) furry animals is limited by tedious off-line rendering, let alone interactive motion control. Computer-generated (CGI) furry animals is limited by tedious off-line rendering, and hence impractical for interactive motion control. In this paper, we present ARTEMIS, a novel neural modeling and rendering pipeline for generating ARTiculated neural pets with appEarance and Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time animation and photo-realistic rendering of furry animals. The core of our ARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient octree based representation for animal animation and fur rendering. The animation then becomes equivalent to voxel level deformation based on explicit skeletal warping. We further use a fast octree indexing an efficient volumetric rendering scheme to generate appearance and density features maps. Finally, we propose a novel shading network to generate high-fidelity details of appearance and opacity under novel poses from appearance and density feature maps. For the motion control module in ARTEMIS, we combines state-of-the-art animal motion capture approach with recent neural character control scheme. We introduce an effective optimization scheme to reconstruct skeletal motion of real animals captured by a multi-view RGB and Vicon camera array. We feed all the captured motion into a neural character control scheme to generate abstract control signals with motion styles. We further integrate ARTEMIS into existing engines that support VR headsets, providing an unprecedented immersive experience where a user can intimately interact with a variety of virtual animals with vivid movements and photo-realistic appearance. Extensive experiments and showcases demonstrate the effectiveness of our ARTEMIS system to achieve highly realistic rendering of NGI animals in real-time, providing daily immersive and interactive experience with digital animals unseen before.\nOverview Given multi-view RGBA images of traditional modeled animals rendered in canonical space, we first extract a sparse voxel grid and allocate a corresponding feature look-up table as a compact representation, together with an octree for quick feature indexing. Then we pose the character to training poses using the rig of the traditional animal asset and conduct efficient volumetric rendering to generate view-dependent appearance feature maps and coarse opacity maps. We next decode them into high quality appearance and opacity images with the convolutional neural shading network. We further adopt an adversarial training scheme for high frequency details synthesis. Results We show our synthesized RGBA results of different neural volumetric animals in representative motions.\nInteractive NGI Animals in VR we exhibit our rendering results in VR applications including different level interactions and different perspectives. First line ‘viewing’ shows the vivid virtual panda we observed from the third and first view(VR headset), respectively. We can clearly see good fur effects even in VR headsets. Second line ‘Instructions’ shows the low-level control animation results. We can explicitly drive our neural pets using control signals like ‘Jump’,‘Move’ and ‘Sit’. ‘Commands’ illustrates our high level control patterns ‘Go to’. The user points to a 3D location in virtual space and the wolf reaches the target destination automatically. We can also call back the wolf by waving. Note that the speed of the movement is also controlled by user. In ‘Companion’, our virtual pet will follow and accompany the user like a real pet. Finally, ‘Exploring’ shows our free mode, when no command is given, the animal can take any reasonable movements, exploring the virtual world themselves.\nAcknowledgements The authors would like to thank Junyu Zhou and Ya Gao from DGene Digital Technology Co., Ltd. for processing the CGI animals models and motion capture data. Besides, we thank Zhenxiao Yu and Heyang Li from ShanghaiTech University for producing a supplementary video and figures.\nThis work was supported by NSFC programs (61976138, 61977047), the National Key Research and Development Program (2018YFB2100500), STCSM (2015F0203-000-06) and SHMEC (2019-01-07-00-01-E00003).\nCitation If you find our code or paper helps, please consider citing:\n@article{10.1145/3528223.3530086, author = {Luo, Haimin and Xu, Teng and Jiang, Yuheng and Zhou, Chenglin and Qiu, Qiwei and Zhang, Yingliang and Yang, Wei and Xu, Lan and Yu, Jingyi}, title = {Artemis: Articulated Neural Pets with Appearance and Motion Synthesis}, year = {2022}, issue_date = {July 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {41}, number = {4}, issn = …","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"827f5823c30dbac33eaf073071457650","permalink":"http://localhost:1313/publication/artemis/","publishdate":"2022-02-21T00:00:00Z","relpermalink":"/publication/artemis/","section":"publication","summary":"We present ARTEMIS, a novel neural modeling and rendering pipeline for generating ARTiculated neural pets with appEarance and Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time animation and photo-realistic rendering of furry animals.","tags":[],"title":"Artemis: Articulated Neural Pets with Appearance and Motion Synthesis","type":"publication"},{"authors":["Anqi Pang","Xin Chen","Haimin Luo","Minye Wu","Jingyi Yu","Lan Xu"],"categories":[],"content":"","date":1632818459,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632818459,"objectID":"d14dfc15f4d0bbcf54efd892dba4245e","permalink":"http://localhost:1313/publication/fnhr/","publishdate":"2021-09-28T16:40:59+08:00","relpermalink":"/publication/fnhr/","section":"publication","summary":"We propose a few-shot neural human rendering approach (FNHR) from only sparse RGBD inputs","tags":[],"title":"Few-shot Neural Human Performance Rendering from Sparse RGBD Videos","type":"publication"},{"authors":["Haimin Luo","Anpei Chen","Qixuan Zhang","Bai Pang","Minye Wu","Lan Xu","Jingyi Yu"],"categories":[],"content":" Overview Given multi-view RGBA images, we use an SFS to infer proxy geometric for Efficient Ray Sampling. For each sample point in the volume space, the position and direction are feeding to an MLP based feature prediction network to represent the object at a global level. We next concatenate nearby rays into local feature patches and decoded them into RGB and matte with the convolutional volume renderer. An adversarial training strategy is used on the final output to encourage fine surface details. In the reference period, we render the entire image at once rather than per patch rendering.\nOur Results Acknowledgements This work was supported by NSFC programs (61976138, 61977047), the National Key Research and Development Program (2018YFB2100500), STCSM (2015F0203-000-06) and SHMEC (2019-01-07-00-01-E00003).\nCitation @INPROCEEDINGS {9466273, author = {H. Luo and A. Chen and Q. Zhang and B. Pang and M. Wu and L. Xu and J. Yu}, booktitle = {2021 IEEE International Conference on Computational Photography (ICCP)}, title = {Convolutional Neural Opacity Radiance Fields}, year = {2021}, volume = {}, issn = {}, pages = {1-12}, keywords = {training;photography;telepresence;image color analysis;computational modeling;entertainment industry;image capture}, doi = {10.1109/ICCP51581.2021.9466273}, url = {https://doi.ieeecomputersociety.org/10.1109/ICCP51581.2021.9466273}, publisher = {IEEE Computer Society}, address = {Los Alamitos, CA, USA}, month = {may} } ","date":1630686522,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630686522,"objectID":"6e7fc3268da46f16bf6fa00010ecb753","permalink":"http://localhost:1313/publication/convnerf/","publishdate":"2021-09-04T00:28:42+08:00","relpermalink":"/publication/convnerf/","section":"publication","summary":"We propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects with high feaqurncy details.","tags":[],"title":"Convolutional Neural Opacity Radiance Fields","type":"publication"},{"authors":["Quan Meng","Anpei Chen","Haimin Luo","Minye Wu","Hao Su","Lan Xu","Xuming He","Jingyi Yu"],"categories":[],"content":"","date":1629103266,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629103266,"objectID":"49295fb3772e8f7420f0c5b8caf2323b","permalink":"http://localhost:1313/publication/gnerf/","publishdate":"2021-08-16T16:41:06+08:00","relpermalink":"/publication/gnerf/","section":"publication","summary":"We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses.","tags":[],"title":"Gnerf: Gan-based neural radiance field without posed camera","type":"publication"}]